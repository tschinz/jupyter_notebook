{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple nim - AI\n",
    "I have prepared something cool today. Let’s implement a program with [a simple] artificial intelligence.\n",
    "\n",
    "The task that the program has to learn is a game called [simple Nim](https://en.wikipedia.org/wiki/Nim). There are two players who alternately take 1, 2 or 3 objects from a heap and the player who takes the last object wins.\n",
    "\n",
    "Remember [game theory](https://en.wikipedia.org/wiki/Game_theory)? For any finite deterministic game of two players and without draws there exists a winning strategy for one of the players. Can you find the strategy on your own?\n",
    "\n",
    "There are many definitions of what intelligence is or what artificial intelligence should be [and nobody actually knows]. Intuitively, we would expect the program to learn from an experience and gradually improve its performance.\n",
    "\n",
    "However, rather counter-intuitive is a fact that to simulate human behaviour and/or human skills we only need very little of statistics. People often refuse to accept that humans would be so simple. And humans are not simple, but their behaviour very often is.\n",
    "\n",
    "To implement AI player, let’s set the following rules.\n",
    "\n",
    "* For each size of the heap keep the total number of wins per objects taken. For example, when heap size is `20`, remember that player won `100x` when took `1` object, won `200x` when took `2` objects, and won `1x` when took `3` objects.\n",
    "* In the next game randomize each move. In the example above, player should take `1` object with probability `100/(100+200+1)`, take `2` objects with probability `200/301`, and take `3` objects with probability `1/301`.\n",
    "* When the player wins the game, increase number of wins at each move played.\n",
    "* When the player looses the game, increase number of wins for the two alternative moves.\n",
    "\n",
    "The code below does exactly what I have just described. When class `Player` is called to play, it takes the current distribution for the heap size and randomizes move. After the game has finished, method `learn` updates distributions based on the learning rules (3) and (4).\n",
    "\n",
    "The rule (3) is crystal clear, player won. The rule (4) allows the player to learn even from the loss. There is also a dirty trick [called normalization] I use to speed the learning up, but it’s the least important part of the code.\n",
    "\n",
    "What happens if we let the AI player learn through 100.000 games against different opponents?\n",
    "expert opponent\n",
    "\n",
    "The opponent knows and plays the winning strategy. If there is none, she randomizes move. [While this might look as the best strategy, it’s not — can you find a better strategy based on the examples I provided?]\n",
    "```\n",
    "10000 games, W/L ratio 0.0081\n",
    "20000 games, W/L ratio 0.0138\n",
    "30000 games, W/L ratio 0.012\n",
    "40000 games, W/L ratio 0.0344\n",
    "50000 games, W/L ratio 0.0386\n",
    "60000 games, W/L ratio 0.1356\n",
    "70000 games, W/L ratio 0.4653\n",
    "80000 games, W/L ratio 0.4978\n",
    "90000 games, W/L ratio 0.4988\n",
    "100000 games, W/L ratio 0.4995\n",
    "```\n",
    "You can see via win/loose ratio the AI gradually improves its performance. In the last 10.000 games AI won almost exactly half of the games.\n",
    "\n",
    "![day90-simple_nim_ai_1](resource/day90-simple_nim_ai_1.png)\n",
    "\n",
    "Check what AI thinks about the game. X-axis contains heap size and y-axis contains probability distribution. E.g. when the heap size is 5, AI will almost always take 1 object. For the heaps where it can’t win the distribution is almost uniform [and it’s not just because of a dirty trick I do to speed the learning up], hence AI has no preference.\n",
    "random opponent\n",
    "\n",
    "The opponent simply takes random number of objects without thinking.\n",
    "```\n",
    "10000 games, W/L ratio 0.8735\n",
    "20000 games, W/L ratio 0.9495\n",
    "30000 games, W/L ratio 0.959\n",
    "40000 games, W/L ratio 0.9597\n",
    "50000 games, W/L ratio 0.96\n",
    "60000 games, W/L ratio 0.9678\n",
    "70000 games, W/L ratio 0.962\n",
    "80000 games, W/L ratio 0.9656\n",
    "90000 games, W/L ratio 0.9654\n",
    "100000 games, W/L ratio 0.9639\n",
    "```\n",
    "AI crushed the opponent, even though there’s a non-negligible chance to win the game even if you simply guess.\n",
    "\n",
    "![day90-simple_nim_ai_2](resource/day90-simple_nim_ai_2.png)\n",
    "\n",
    "What AI thinks about the game know? It’s not surprising to see much wider distributions. That’s because AI learns from wins and the larger the heap is, the higher was the chance to win regardless of what was played.\n",
    "\n",
    "## take-3 opponent\n",
    "\n",
    "What about the opponent that always takes 3 objects?\n",
    "```\n",
    "10000 games, W/L ratio 0.9743\n",
    "20000 games, W/L ratio 0.9979\n",
    "30000 games, W/L ratio 0.9991\n",
    "40000 games, W/L ratio 0.999\n",
    "50000 games, W/L ratio 0.9996\n",
    "60000 games, W/L ratio 1.0\n",
    "70000 games, W/L ratio 0.9999\n",
    "80000 games, W/L ratio 1.0\n",
    "90000 games, W/L ratio 1.0\n",
    "100000 games, W/L ratio 1.0\n",
    "```\n",
    "A decisive victory! This is clearly very bad strategy without any chance on random win.\n",
    "\n",
    "![day90-simple_nim_ai_2](resource/day90-simple_nim_ai_2.png)\n",
    "\n",
    "Look at the chart when the heap size is `6`, AI takes `1` or `2` objects with an equal chance. Both moves lead to a quick win so it doesn’t really matter. But the move is not deterministic which complicates identification of AI’s strategy.\n",
    "\n",
    "That seems to be very close to how humans think. The program doesn’t think, but it is still able to simulate human-like behaviour.\n",
    "\n",
    "The AI player knows nothing about the game. It only deploys statistics to adapt to a strategy of its opponent. While it’s probably not what you might imagine under term artificial intelligence, it’s exactly how it works.\n",
    "\n",
    "And here’s what you can do. Download the notebook, examine my code and write your own opponent. Then let the AI to discover weak points of your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## player: AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "\n",
    "    def __init__(self, heap):\n",
    "        self.history = {}\n",
    "        self.distribution = np.ones((heap + 1, 3), dtype=int)\n",
    "        self.cutoff = 1000\n",
    "\n",
    "    def __call__(self, heap):\n",
    "        # randomize move based on previous games\n",
    "        dist = self.distribution[heap].cumsum()\n",
    "        rnd = np.random.randint(dist[2])\n",
    "        move = 1 if rnd < dist[0] else 2 if rnd < dist[1] else 3\n",
    "        \n",
    "        # store move in history\n",
    "        self.history[heap] = min(heap, move)\n",
    "        \n",
    "        return self.history[heap]\n",
    "\n",
    "    def learn(self, winner):\n",
    "        # update move distribution\n",
    "        for heap, move in self.history.items():\n",
    "            if winner is self:\n",
    "                self.distribution[heap][move - 1] += 1\n",
    "            else:\n",
    "                self.distribution[heap][move - 1] -= 1\n",
    "                self.distribution[heap] += 1\n",
    "\n",
    "        # normalize distribution to speed learning up\n",
    "        normalize = np.argwhere(self.distribution.sum(axis=1) > self.cutoff)\n",
    "        for heap in normalize:\n",
    "            self.distribution[heap] -= self.distribution[heap].min() - 1\n",
    "\n",
    "        # reset game history\n",
    "        self.history = {}\n",
    "    \n",
    "    def strategy(self):\n",
    "        distribution = self.distribution[1:]\n",
    "        return distribution.T / distribution.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expert_opponent(heap):\n",
    "    return heap % 4 or min(heap, np.random.randint(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_opponent(heap):\n",
    "    return min(heap, np.random.randint(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_n_opponent(take):\n",
    "    return lambda heap: min(heap, take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(heap, player, opponent):\n",
    "    players = player, opponent\n",
    "    wins = 0\n",
    "\n",
    "    for game in range(100001):\n",
    "        # update plot periodically\n",
    "        if game % 10000 == 0:\n",
    "            print(game, 'games, W/L ratio', wins / 10000)\n",
    "            wins = 0\n",
    "\n",
    "        # a single game\n",
    "        h = heap\n",
    "        while h:\n",
    "            h -= players[0](h)\n",
    "            players = players[1], players[0]\n",
    "\n",
    "        winner = players[1]\n",
    "        wins += winner is player\n",
    "            \n",
    "        # let player learn\n",
    "        player.learn(winner)\n",
    "        \n",
    "    # plot distribution\n",
    "    plot_strategy(heap, player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_strategy(heap, player):\n",
    "    output_notebook()\n",
    "\n",
    "    # data\n",
    "    take_1, take_2, take_3 = player.strategy()\n",
    "    take_2 += take_1\n",
    "    take_3 += take_2\n",
    "    kwargs = {'x': range(1, heap + 1), 'width': .8}\n",
    "\n",
    "    # plot\n",
    "    plot = figure(plot_width=600, plot_height=400)\n",
    "    plot.vbar(**kwargs, bottom=0, top=take_1, legend='take 1', color='#a44444')\n",
    "    plot.vbar(**kwargs, bottom=take_1, top=take_2, legend='take 2', color='#88a888')\n",
    "    plot.vbar(**kwargs, bottom=take_2, top=take_3, legend='take 3', color='#ccccac')\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEAP = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "play(HEAP, Player(HEAP), expert_opponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "play(HEAP, Player(HEAP), random_opponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "play(HEAP, Player(HEAP), take_n_opponent(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "play(HEAP, Player(HEAP), take_n_opponent(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
