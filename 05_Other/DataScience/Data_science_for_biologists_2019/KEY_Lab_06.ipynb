{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Classification in Scikit-Learn\n",
    "**Data Science for Biologists** &#8226; University of Washington &#8226; BIOL 419/519 &#8226; Winter 2019\n",
    "\n",
    "Course design and lecture material by [Bingni Brunton](https://github.com/bwbrunton) and [Kameron Harris](https://github.com/kharris/). Lab design and materials by [Eleanor Lutz](https://github.com/eleanorlutz/), with helpful comments and suggestions from Bing and Kam.\n",
    "\n",
    "### Table of Contents\n",
    "1. Review of importing and inspecting data\n",
    "2. Split a dataset into a training and test set\n",
    "3. Train a machine learning classifier using scikit-learn\n",
    "4. Bonus exercises\n",
    "\n",
    "### Helpful resources\n",
    "- [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas\n",
    "- [An introduction to machine learning with Scikit-Learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "- [Scikit-Learn user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Scikit-Learn Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116) by Python for Data Science\n",
    "\n",
    "### Data\n",
    "- The data in this lab is originally from [USA Forensic Science Service](https://archive.ics.uci.edu/ml/datasets/Glass+Identification) and was edited for teaching purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's lab requires plotting several different classifications in different colors, so set the default matplotlib style to a colorblind-friendly setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 Part 1: Review of importing and inspecting data\n",
    "\n",
    "This week's data is from the USA Forensic Science Service, and contains information about 214 samples from seven different types of glass (vehicle windows, tableware, headlamps, etc). By analyzing various properties of the glass, such as the refractive index or the proportion of different chemical elements (Na, Mg, Al, etc.), this dataset can be used to predict the source of unknown glass found at crime scenes. \n",
    "\n",
    "**Exercise 1:** Read in the dataset in the file `\"./data/Lab_06/glass_properties_data.csv\"` as a `Pandas` dataframe called `df`. Display the `head()` of the data and `describe()` the dataframe to make sure your `df` variable was imported properly and has the expected columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Glass_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Glass_Type\n",
       "0   1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0           1\n",
       "1   2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0           1\n",
       "2   3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0           1\n",
       "3   4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0           1\n",
       "4   5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Glass_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>107.500000</td>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.920648</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.250000</td>\n",
       "      <td>1.516523</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>107.500000</td>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>160.750000</td>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID          RI          Na          Mg          Al          Si  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean   107.500000    1.518365   13.407850    2.684533    1.444907   72.650935   \n",
       "std     61.920648    0.003037    0.816604    1.442408    0.499270    0.774546   \n",
       "min      1.000000    1.511150   10.730000    0.000000    0.290000   69.810000   \n",
       "25%     54.250000    1.516523   12.907500    2.115000    1.190000   72.280000   \n",
       "50%    107.500000    1.517680   13.300000    3.480000    1.360000   72.790000   \n",
       "75%    160.750000    1.519157   13.825000    3.600000    1.630000   73.087500   \n",
       "max    214.000000    1.533930   17.380000    4.490000    3.500000   75.410000   \n",
       "\n",
       "                K          Ca          Ba          Fe  Glass_Type  \n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     0.497056    8.956963    0.175047    0.057009    2.780374  \n",
       "std      0.652192    1.423153    0.497219    0.097439    2.103739  \n",
       "min      0.000000    5.430000    0.000000    0.000000    1.000000  \n",
       "25%      0.122500    8.240000    0.000000    0.000000    1.000000  \n",
       "50%      0.555000    8.600000    0.000000    0.000000    2.000000  \n",
       "75%      0.610000    9.172500    0.000000    0.100000    3.000000  \n",
       "max      6.210000   16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/Lab_06/glass_properties_data.csv\")\n",
    "\n",
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 Part 2: Split the dataset into a training set and test set\n",
    "\n",
    "Using this dataset, we would like to create a machine learning classifier that can be used to categorize unknown glass samples. \n",
    "\n",
    "A machine learning classifier is trained based on one particular set of data. A separate dataset is used to evaluate the accuracy of the classifier. For this lab we'll use 70% of the data for training and save the remaining 30% for testing the classifier. This means that we first need to separate a random 70% of the data into a `train_data` variable, and the other 30% into a different `test_data` variable. \n",
    "\n",
    "<img src=\"http://ataspinar.com/wp-content/uploads/2016/12/classification.png\">\n",
    "\n",
    "*Credit:* A schematic overview of the classification process, by Ahmet Taspinar\n",
    "\n",
    "**Exercise 2:** Each sample in this dataset has an `ID` number from 1 to 214. Create a `numpy` vector called `indices` that contains all of the index numbers (all integers from 1 to 214). We will use this vector to shuffle the data `ID`s and separate the `ID`s into training and testing groups. Print the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(1, 215)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** The `np.random.shuffle()` function can be used to randomly shuffle everything stored in a list or `numpy` array. Run the following code block and confirm that the ID numbers stored in `indices` has indeed been shuffled: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98 140  35 192 166 184 146 143 128 117 173 201  93  53  81 119  31  75\n",
      " 110 131 154 101 182 133 161 116  82  67  78 103 207 104  80  24 191   1\n",
      " 155  17 205  52 107   4 180 169 195 145 114  59  86  20  87 204   7  83\n",
      " 137 156 186  28 147 167  46 177 210 122 157  32 115 118 132  13 136  63\n",
      " 199  73 111  60  58   5  22 209  95 165 170  49 185 194  15 190 142  61\n",
      "  92 164 181  96 135 188  21  26 174 126 183  47  11 152  51 187 102  30\n",
      " 148 214 121 206  69  77  10  84 160 162  36 213  16  23  71  34  76 109\n",
      " 141  25 171 197  90  18 203  79 139 150 163 124  74  14 200 212 158 113\n",
      " 138 198 151  55 129  56  37  64  39  85 123  70 202 208  72  44  48 172\n",
      "   3 168  50  27 127  42 149  43  38  88  65 108 105  40 175 106  57 178\n",
      " 193   2 120  62  12 189 153  29  33 196  19  66 100   9  99  91   6 112\n",
      "  89 125 134 176  68 211  54 144  94 179 159   8 130  41  97  45]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** If we take the first 70% of the shuffled `indices` array, this should contain all of the data ID numbers to use for our training dataset. Create a variable called `train_indices` that contains just the first 70% of `indices`. Create another variable called `test_indices` that contains the rest of `indices`. If `indices` is not divisible into clean 70%-30% segments, you can round to the nearest number. Print the length of `train_indices` and `test_indices`. Print `test_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_indices is: 149\n",
      "Length of test_indices is: 65\n",
      "[ 56  37  64  39  85 123  70 202 208  72  44  48 172   3 168  50  27 127\n",
      "  42 149  43  38  88  65 108 105  40 175 106  57 178 193   2 120  62  12\n",
      " 189 153  29  33 196  19  66 100   9  99  91   6 112  89 125 134 176  68\n",
      " 211  54 144  94 179 159   8 130  41  97  45]\n"
     ]
    }
   ],
   "source": [
    "split_index = int(len(indices)*0.7)\n",
    "train_indices = indices[0:split_index]\n",
    "test_indices = indices[split_index:]\n",
    "\n",
    "print(\"Length of train_indices is:\", len(train_indices))\n",
    "print(\"Length of test_indices is:\", len(test_indices))\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two variables that hold the `ID` numbers for a randomized selection of 70% and 30% of the data. We can now use these `ID` numbers to separate the data into training and test sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = df[df[\"ID\"].isin(train_indices)]\n",
    "test_data = df[df[\"ID\"].isin(test_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Evaluate the `train_data` dataframe you just constructed to make sure it looks correct, using the following steps:\n",
    "- Print the number of rows in `train_data` and the original data. \n",
    "- Print the percentage of rows in the original data is included in `train_data`. Is this close to the expected 70%?\n",
    "- Display the `head()` of the `train_data` dataframe and see if the data is a random selection of the original. *Hint:* Look at the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in original data is 214\n",
      "Rows in training data is 149 which is 69.62616822429906 % of the original data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Glass_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.51743</td>\n",
       "      <td>13.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.14</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.51755</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       RI     Na    Mg    Al     Si     K    Ca   Ba    Fe  Glass_Type\n",
       "0   1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.00           1\n",
       "3   4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.00           1\n",
       "4   5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.00           1\n",
       "6   7  1.51743  13.30  3.60  1.14  73.09  0.58  8.17  0.0  0.00           1\n",
       "9  10  1.51755  13.00  3.60  1.36  72.99  0.57  8.40  0.0  0.11           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows in original data is\", len(df))\n",
    "print(\"Rows in training data is\", len(train_data), \n",
    "      \"which is\", 100*len(train_data)/len(df), \"% of the original data\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Evaluate the `test_data` dataframe similarly to make sure it looks correct:\n",
    "- Print the number of rows in `test_data`. \n",
    "- Print the percentage of rows in the original data is included in `test_data`. Is this close to the expected 30%?\n",
    "- Display the `head()` of the `test_data` dataframe and see if the data is a random selection of the original.\n",
    "- Does the number of rows in `test_data` and `train_data` add up to the number of rows in the full data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test data is 65 which is 30.373831775700936 % of the original data\n",
      "Test and train datasets add up to 214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Glass_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.51596</td>\n",
       "      <td>12.79</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>72.97</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.51756</td>\n",
       "      <td>13.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.05</td>\n",
       "      <td>73.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.51918</td>\n",
       "      <td>14.04</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.37</td>\n",
       "      <td>72.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       RI     Na    Mg    Al     Si     K    Ca   Ba    Fe  Glass_Type\n",
       "1   2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.00           1\n",
       "2   3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.00           1\n",
       "5   6  1.51596  12.79  3.61  1.62  72.97  0.64  8.07  0.0  0.26           1\n",
       "7   8  1.51756  13.15  3.61  1.05  73.24  0.57  8.24  0.0  0.00           1\n",
       "8   9  1.51918  14.04  3.58  1.37  72.08  0.56  8.30  0.0  0.00           1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows in test data is\", len(test_data), \n",
    "      \"which is\", 100*len(test_data)/len(df), \"% of the original data\")\n",
    "print(\"Test and train datasets add up to\", len(test_data)+len(train_data))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data using Pandas\n",
    "\n",
    "Instead of dividing the data into training and test sets manually, `pandas` also has a builtin function to sample a random subset of the dataset (as do other libraries including `scikit-learn`. The previous data splitting can also be done in two lines (for future reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.7)\n",
    "test_data = df.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 Part 3: Train a machine learning classifier using scikit-learn\n",
    "\n",
    "Like `numpy` or `pandas`, [scikit-Learn](https://scikit-learn.org/stable/) is a python library for machine learning. In today's lab we will make a Linear Discriminant Analysis classifier by importing this function from the `sklearn.discriminant_analysis` module. \n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "In class we discussed the Linear Discriminant Analysis method for two variables. The scikit-learn library can also extrapolate this principle to run a LDA on many variables. To run a LDA on our dataset we will import the `LDA` function from the Scikit learn (`sklearn`) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `LinearDiscriminantAnalysis` function we need to first separate the data into the data itself and the classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57     1\n",
       "178    6\n",
       "184    6\n",
       "28     1\n",
       "115    2\n",
       "Name: Glass_Type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.51824</td>\n",
       "      <td>12.87</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.51829</td>\n",
       "      <td>14.46</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.62</td>\n",
       "      <td>72.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.51768</td>\n",
       "      <td>12.56</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.43</td>\n",
       "      <td>73.15</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.51846</td>\n",
       "      <td>13.41</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.33</td>\n",
       "      <td>72.38</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RI     Na    Mg    Al     Si     K    Ca   Ba   Fe\n",
       "57   1.51824  12.87  3.48  1.29  72.95  0.60  8.43  0.0  0.0\n",
       "178  1.51829  14.46  2.24  1.62  72.38  0.00  9.26  0.0  0.0\n",
       "184  1.51115  17.38  0.00  0.34  75.41  0.00  6.65  0.0  0.0\n",
       "28   1.51768  12.56  3.52  1.43  73.15  0.57  8.54  0.0  0.0\n",
       "115  1.51846  13.41  3.89  1.33  72.38  0.51  8.28  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_class = train_data[\"Glass_Type\"] # known classification answers\n",
    "train_vals = train_data.drop([\"Glass_Type\", \"ID\"], axis=1) # corresponding values \n",
    "# make sure to drop the ID since it is not a characteristic of the glass\n",
    "\n",
    "display(train_class.head())\n",
    "display(train_vals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Similarly, split the `test_data` test dataset into a variable called `test_class` that holds all of the `Glass_Type` classifications, and another variable called `test_vals` that contains the corresponding measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "9    1\n",
       "Name: Glass_Type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.51596</td>\n",
       "      <td>12.79</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>72.97</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.51743</td>\n",
       "      <td>13.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.14</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.51756</td>\n",
       "      <td>13.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.05</td>\n",
       "      <td>73.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.51755</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba    Fe\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.00\n",
       "5  1.51596  12.79  3.61  1.62  72.97  0.64  8.07  0.0  0.26\n",
       "6  1.51743  13.30  3.60  1.14  73.09  0.58  8.17  0.0  0.00\n",
       "7  1.51756  13.15  3.61  1.05  73.24  0.57  8.24  0.0  0.00\n",
       "9  1.51755  13.00  3.60  1.36  72.99  0.57  8.40  0.0  0.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_class = test_data[\"Glass_Type\"]\n",
    "test_vals = test_data.drop([\"Glass_Type\", \"ID\"], axis=1)\n",
    "\n",
    "display(test_class.head())\n",
    "display(test_vals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training data is separated into the values and the known classifier values, these two dataframes can be used to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(train_vals, train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the classifier, we can use it to predict the types of glass (classes) for the training data, and see what proportion of the predictions matched the correct answers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is: 0.66\n"
     ]
    }
   ],
   "source": [
    "train_score = classifier.score(train_vals, train_class)\n",
    "print(\"Training score is:\", train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier can choose between 7 different glass types, so anything above 1/7 (or 14.29%) is better than chance. The classifier should be performing much better than 14% on the training dataset we scored above. \n",
    "\n",
    "But the real evaluation of the classifier is to test its accuracy in predicting the glass classes for data it has never seen before (the test dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score is: 0.640625\n"
     ]
    }
   ],
   "source": [
    "test_score = classifier.score(test_vals, test_class)\n",
    "print(\"Test score is:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Compare your test score with two other people in the class. \n",
    "- Did you get the same accuracy values? If not, why do you think this might be the case? Explain briefly as a comment below: \n",
    "- Is the accuracy of your classifier better or worse for the test data or the training data? Why do you think this is? \n",
    "\n",
    "** Answer for Exercise 8:** # Each classifier is trained on a random subset of 70% of the data, so everyone's classifier is different. The accuracy for the test data will typically be worse, because the classifier was not trained on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 Bonus exercises\n",
    "\n",
    "### Comparing different classifiers: LDA vs K-Nearest Neighbors\n",
    "In class we discussed Nearest Neighbor classification, which classifies an unknown data point as belonging to the same class as its nearest neighboring point. K-Nearest Neighbors extrapolates this approach to more than one neighbor. For example, a 3-nearest neighbor classification looks at the three nearest data points, and picks the most common class from among those three neighbors. \n",
    "\n",
    "In `sklearn` a KNN classifier can be imported from the `sklearn.neighbors` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise 1:** Evaluate the accuracy of `KNeighborsClassifier()` using the same training and test dataset as used previously in the lab. Is this classification method better than LDA for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN training score: 0.733333333333 test score: 0.734375\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(train_vals, train_class)\n",
    "\n",
    "train_score = classifier.score(train_vals, train_class)\n",
    "test_score = classifier.score(test_vals, test_class)\n",
    "print(\"KNN training score:\", train_score, \"test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise 2:** Look up the documentation for `sklearn KNeighborsClassifier()` to find out how to specify the number of neighbors to use. Make a scatterplot showing the accuracy of the classifier on the test dataset for a range of n-nearest neighbors (1 to 50). How does the number of nearest neighbors affect classifier accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn6R8YfAy0pcJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3741644a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_n(n):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=n)\n",
    "    classifier.fit(train_vals, train_class)\n",
    "    test_score = classifier.score(test_vals, test_class)\n",
    "    return test_score\n",
    "\n",
    "ns = np.arange(1, 51)\n",
    "ts = [score_n(n) for n in ns]\n",
    "plt.scatter(ns, ts)\n",
    "plt.xlabel(\"n nearest neighbors\")\n",
    "plt.ylabel(\"accuracy on test dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
